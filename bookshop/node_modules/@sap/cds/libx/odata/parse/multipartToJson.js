const { parsers, freeParser, HTTPParser } = require('_http_common')
const { PassThrough, Readable } = require('stream')
const streamConsumers = require('stream/consumers')
const { getBoundary } = require('../utils')

const CRLF = '\r\n'

const parseStream = async function* (body, boundary) {
  const parser = parsers.alloc()

  try {
    const boundaries = [boundary]
    let content_id
    let yielded = 0
    const requests = []
    let idCount = 0

    parser.onIncoming = function (req /*, keepAlive */) {
      // Boundaries encoded as HEAD request
      if (req.method === 'HEAD') {
        if (`/${boundaries.at(-1)}` !== req.url) {
          // TODO: error ?
        }
        // Leave current boundary
        if (`/${boundaries.at(-1)}--` === req.url) {
          boundaries.pop()
        }
        const newBoundary = getBoundary(req)
        if (newBoundary) boundaries.push(newBoundary)
        content_id = req.headers['content-id']
        return
      }

      const wrapper = new PassThrough()
      req.pipe(wrapper)

      const request = {
        id: `r${++idCount}`,
        url: req.url,
        method: req.method,
        headers: { ...req.headers },
        body: streamConsumers.json(wrapper).catch(() => {})
      }

      const dependencies = [...req.url.matchAll(/\$(\d+)/g)]
      if (dependencies.length) {
        request.dependsOn = []
        for (const dependency of dependencies) {
          const index = Number.parseInt(dependency[1], 10)
          if (Number.isNaN(index)) continue
          const i = requests.findIndex(r => r.content_id == index) //> prefer content-id
          const id = requests[i > -1 ? i : index - 1].id
          request.dependsOn.push(id)
          request.url = request.url.replace(`$${index}`, `$${id}`)
        }
        if (request.url[1] === '$') request.url = request.url.slice(1)
      }

      if (boundaries.length > 1) request.atomicityGroup = boundaries.at(-1)
      if (content_id) request.content_id = content_id

      requests.push(request)
    }

    parser.initialize(HTTPParser.REQUEST, { type: 'HTTPINCOMINGMESSAGE' })

    if (typeof body === 'string') body = [body]

    const process = chunk => {
      let changed = chunk
        .toString()
        .replace(/^--(.*)$/gm, (_, g) => `HEAD /${g} HTTP/1.1${g.slice(-2) === '--' ? CRLF : ''}`)
        // correct content-length for non-HEAD requests is inserted below
        .replace(/content-length: \d+\r\n/gim, '')
        .replace(/ \$/g, ' /$')

      // HACKS!!!
      // ensure URLs start with slashes
      changed = changed.replaceAll(/\r\n(GET|PUT|POST|PATCH|DELETE) (\w)/g, `\r\n$1 /$2`)
      // add content-length headers
      let lastIndex = 0
      changed = changed.replaceAll(/(\r\n){2,}(.+)[\r\n]+HEAD/g, (match, _, p1, index, original) => {
        const part = original.substring(lastIndex, index)
        lastIndex = index
        return part.match(/(PUT|POST|PATCH)\s\//g) && !part.match(/content-length/i) && !p1.startsWith('HEAD /')
          ? `${CRLF}content-length: ${Buffer.byteLength(p1)}${match}`
          : match
      })
      // remove strange "Group ID" appendix
      changed = changed.split(`${CRLF}Group ID`)[0] + CRLF

      let ret = parser.execute(Buffer.from(changed))

      if (typeof ret !== 'number') {
        if (ret.message === 'Parse Error') {
          // console.trace(ret, ret.bytesParsed ? `\n\nin:\n${changed.substr(0, ret.bytesParsed + 1)}\n\n` : '')
          ret.statusCode = 400
          ret.message = `Error while parsing batch body at position ${ret.bytesParsed}: ${ret.reason}`
        }
        throw ret
      }
    }

    let leftover = ''
    for await (let chunk of body) {
      // Ensure that the whole boundary is inside the current chunk
      chunk = `${leftover}${chunk}`
      const lastBoundary = chunk.lastIndexOf('--')
      const lastCRLF = chunk.lastIndexOf(CRLF)
      if (lastBoundary > lastCRLF && lastBoundary + 2 < chunk.length) {
        leftover = chunk.slice(lastBoundary)
        chunk = chunk.slice(0, lastBoundary)
      } else {
        leftover = ''
      }
      process(chunk)

      // Drain request
      for (; yielded < requests.length; yielded++) {
        // TODO: remove should be consumed by protocol adapter itself
        requests[yielded].body = await requests[yielded].body
        if (requests[yielded].body === undefined) {
          delete requests[yielded].body
        }
        yield requests[yielded]
      }
    }

    // Process any leftovers
    if (leftover) {
      process(leftover)

      // Drain request
      for (; yielded < requests.length; yielded++) {
        // TODO: remove should be consumed by protocol adapter itself
        requests[yielded].body = await requests[yielded].body
        if (requests[yielded].body === undefined) {
          delete requests[yielded].body
        }
        yield requests[yielded]
      }
    }
  } finally {
    freeParser(parser)
  }
}

// Normalize
module.exports = async (body, boundary) => {
  const ret = {
    requests: []
  }

  // This logic would ultimately be inside the json batch processor
  // for await supports both async iterator and normal iterators (e.g. any Array)
  for await (const request of Readable.from(parseStream(body, boundary))) {
    ret.requests.push(request)
  }

  return ret
}
